\chapter{Introduction}\label{introduction}

Deep neural networks have achieved impressive performance across domains such as computer vision and natural language processing, yet they remain surprisingly brittle. Small, carefully chosen perturbations to correctly classified inputs can cause state-of-the-art models to fail with high confidence \citep{szegedy2014intriguing, goodfellow2015explaining}. This phenomenon of adversarial examples has motivated a large body of work on robustness and formal verification of neural networks, particularly for safety-critical applications where such failures are unacceptable.

In parallel, formal methods researchers have developed increasingly powerful verification techniques for neural networks and other machine learning components. Tools based on satisfiability modulo theories, abstract interpretation, and specialized solvers such as Reluplex have demonstrated that it is feasible to prove non-trivial safety properties of neural controllers in restricted settings. However, most existing tools are tailored to specific architectures or input domains, and are typically applied \'a posteriori, i.e. after a model has been trained. This separation between training and verification can lead to an expensive iterate-and-fix cycle, and makes it difficult to guarantee that the model actually used in deployment satisfies the same specification that was verified \citep{reluplex2017efficient, casadio2022neural}.

Property-driven machine learning has emerged as a promising response to this challenge. Instead of viewing formal specifications as something checked only after training, property-driven approaches integrate logical constraints directly into the learning process. Differentiable logics allow one to encode arbitrary first-order properties as loss terms that guide optimisation, while adversarial training-style methods can approximate worst-case violations within regions of interest. Recent work by Flinkow et al. \citep{flinkow2025property} proposes a general framework for property-driven training that unifies these ideas: logical specifications are compiled into differentiable objectives, and training is steered towards models that satisfy both data-driven and specification-driven requirements.

The Vehicle project \citep{daggitt2025vehicle} takes a complementary perspective focused on the specification and verification side. Vehicle is a domain-specific language and toolchain for specifying properties of neural and neuro-symbolic programs once, then compiling them to multiple back-ends, including SMT solvers, interactive theorem provers, and specialised neural network verifiers. Vehicle aims to bridge the ``embedding gap'' between machine learning frameworks and formal verification tools by providing a common verification condition language, together with type-safe compilation paths to different verification back-ends.

Despite this progress, there remains a gap between property-driven training frameworks and general-purpose verification condition languages such as Vehicle. On the one hand, frameworks like ANTONIO \citep{casadio2023antonio} demonstrate how domain-specific benchmarks and pipelines can be built to make verification of NLP models practical, by carefully shaping datasets and network architectures to match the capabilities of existing verifiers. On the other hand, Vehicle focuses on specifying properties in an abstract, solver-independent way, but its Python bindings have historically been geared more towards offline verification and experimentation than towards being directly usable as training-time constraints in modern deep learning workflows.

This research project addresses this gap by updating and extending the Python bindings of Vehicle so that the same formally specified property can be used consistently for both training and verification. Concretely, it realigns the Python-side abstract syntax tree with the current Vehicle compiler output, streamlines and internalises the abstraction layer that mediates between Vehicle programs and differentiable back-ends, and provides coherent TensorFlow and PyTorch backends that interpret Vehicle's loss logic as differentiable loss functions. The work also separates the command-line compilation utilities from the loss machinery, thereby avoiding unnecessary ML-framework imports for verification-only workflows while still offering optional TensorFlow and PyTorch extras for property-driven training. On top of this architecture, the project delivers a high-level Python API that allows practitioners to load a Vehicle specification, obtain loss functions corresponding to named properties, and integrate them directly into standard training loops.

\section*{Research Question}

The central question of this research project is:

\begin{quote}
	    	\textbf{How can Vehicle's existing Python bindings be extended so that they correctly reflect the current compiler and loss logic, and support quantifiers and their samplers for use in both training and verification?}
\end{quote}

This question is motivated by the observation that, while the overall structure of the Python bindings and JSON AST parsing for Vehicle already existed, changes in the core language (in particular around quantified search and loss constructs) had introduced a mismatch between the compiler and its Python interface. In collaboration with the core Vehicle developers, this research project therefore focuses on (i) realigning the Python AST and abstraction layer with the current compiler, (ii) co-designing and implementing the representation of \inlinecode{SearchRatTensor} nodes and the corresponding sampler interfaces needed for property-driven training \citep{flinkow2025property}, and (iii) demonstrating that, once these pieces are in place, the same formally specified properties can be exercised both as differentiable losses in TensorFlow/PyTorch and as verification conditions compiled to existing back-ends. The resulting architecture contributes an incremental but necessary step towards workflows in which specification, training, and verification remain technically and semantically aligned.
