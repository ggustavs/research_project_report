\chapter{Approach}\label{approach}

In this chapter I describe how I aligned and extended Vehicle's Python bindings so that they correctly reflect the current Vehicle compiler and loss logic, and how I co-designed the representation and sampling semantics of search-based loss constructs such as \texttt{SearchRatTensor}. The focus is on the concrete engineering decisions that were necessary to make Vehicle specifications usable as training-time constraints in TensorFlow and PyTorch, while preserving Vehicle as the single source of truth for the logical properties.

\section{Overview of the Existing Design}

Before making changes, I analysed the state of the Python bindings on the \texttt{dev} branch. At a high level, three conceptual layers were already present:

\begin{enumerate}
	\item A JSON AST mirror and loader in \texttt{vehicle\_lang.ast}, which called the Vehicle compiler with \texttt{--json compile loss} and decoded the result into Python dataclasses. However, this module was significantly out of date with respect to the current Vehicle AST.
	\item An abstraction layer in \texttt{compile.abc} that defined backend-agnostic interfaces for builtins and translations from Vehicle programs to some target representation.
	\item A Python translation in \texttt{compile.python} that consumed the AST and produced Python \texttt{ast.Module}s, which were then compiled and executed into a \texttt{declaration\_context} dictionary. This dictionary provided concrete implementations of low-level operations in TensorFlow.
\end{enumerate}

Since the existing structure at a high level was sound, just out of date, and missing key components for quantifiers, I decided to build on top of it where possible.

The approach of this research project is therefore not to invent a new architecture from scratch, but to realign these layers with the current Vehicle compiler, clarify and clean-up existing abstractions, implement quantifier parsing, design an interface for samplers, provide a default sampler, and improve the overall code quality and coherence of the bindings.

\section{AST Ingestion from the Vehicle Compiler}

The first step was to bring Python-side AST ingestion back in sync with the Vehicle compiler, addressing the staleness of the JSON AST mirror described above. In collaboration with the core Vehicle developers, I introduced an updated and refactored internal module \texttt{compile.\_ast} to replace the outdated public \texttt{vehicle\_lang.ast} in the compilation pipeline.

\subsection{Internal AST Module}

The \texttt{compile.\_ast} package provides:

\begin{itemize}
	\item A \texttt{load} function that invokes the Vehicle compiler with the appropriate \texttt{--json compile loss} and \texttt{--logic} options, for a given specification file and list of declarations.
	\item A set of \texttt{\_nodes} dataclasses that mirror the current JSON AST produced by the Vehicle compiler, including constructs for quantifiers and operations the new tensor representation.
	\item A refactored JSON decoder \texttt{\_decode.py} that uses modern Python type annotations and improved error handling, making it easier to evolve alongside the compiler.
\end{itemize}

By prefixing a module or script with an underscore, we signal that it is internal and not part of the public API. This gives us freedom to evolve the AST representation as needed without breaking user code as well as improving the usability of the codebase. % TODO: reference to PEP8

\section{Abstraction Layer}

The second step was to streamline the abstractions that enable the modularity of the framework. On \texttt{dev}, the responsible \texttt{compile.abc} module was public and included overlapping concepts such as \texttt{Builtins} and \texttt{ABCBuiltins}. This made it difficult both to understand and to change. Another issue was that the usage of generic types was inconsistent, specifically, \texttt{vcl.type\_name} was used in multiple places, but the specific module that was imported as \texttt{vcl} varied. This led to confusion about which types were actually being used in different contexts.

\subsection{Internal Abstract Base Class Package}

On my branch I introduced the new internal module \texttt{compile.\_abc} which refines and consolidates this abstraction into four main components:

\begin{description}
	\item[\texttt{\_types.py}] Defines generic type variables for indices, rational numbers, tensors and for the program/declaration/expression types that a translation operates on. This makes the interfaces parametric in the backend, while remaining tied to the structure of the Vehicle AST.
	\item[\texttt{\_builtins.py}] Defines a single \texttt{ABCBuiltins[Index, Rat, Tensor]} interface with a coherent set of abstract methods corresponding to Vehicle's core tensor operations, reductions and dimension handling (e.g. \texttt{RatTensor},
	\\\texttt{ReduceAddRatTensor}, dimension constructors, and tensor constructors such as \texttt{ConstTensor} and \texttt{StackTensor}). This interface serves as the contract that backend implementations (e.g. TensorFlow, PyTorch) must satisfy in order to support Vehicle's loss logic.
	\item[\texttt{\_samplers.py}] Introduces an \texttt{ABCSampler[Index, Tensor]} interface that defines what API a sampler should provide for quantifiers. Its central method \\\texttt{get\_loss} returns a tensor-valued loss that realises the semantics of a search region. A key design choice of the maintainers is to support a wide range of approaches, since the sampler can return 1 or more loss values that are combined with a compiled reduction operation (depending on the logic and quantifier). This allows for strategies such as random sampling, quasi-random sampling, or adversarial attacks to be implemented using the same interface. % TODO: find instances of such strategies in literature
	\item[\texttt{\_translation.py}] Defines a generic \texttt{ABCTranslation[Program, Declaration,\\Expression]} that pattern-matches over the new \texttt{\_ast.\_nodes} types. It has concrete methods for translating programs and declarations, while leaving expression translation abstract. This design allows backends to inherit from \texttt{ABCTranslation} and only implement the expression-level translation logic, while reusing the program/declaration-level logic.
\end{description}

This modular design makes it easy to add more backends in the future if they can satisfy these interfaces.

\section{Sampler Design and Implementation}

A central part of my contribution lies in the design and implementation of how \texttt{SearchRatTensor} nodes and their associated sampling behaviour are represented on the Python side. This was done in close collaboration with the Vehicle maintainers.

\subsection{Requirements from Property-Driven Training}

From the perspective of property-driven machine learning \citep{flinkow2025property}, in the case of the \texttt{forall} quantifier, \texttt{SearchRatTensor} captures a search over a region of the input space (described by lower and upper bounds) returning the loss evaluated on worst-case violations of a specification (best-case for the \texttt{exists} quantifier). For training, the sampler needs to:

\begin{itemize}
	\item Find points within the specified search region.
	\item Evaluate the loss objective at those points.
\end{itemize}

To fulfil these requirements, especially with strategies like adversarial attacks which require gradient information, we had to provide more information than just the bounds and loss function to the sampler. Therefore, we designed the sampler interface to accept:

\begin{enumerate}
	\item The dimensions and their bounds.
	\item A representation of the loss objective as a callable function.
	\item A flag indicating whether to minimise or maximise the loss. This is needed because different logics may have different absolute truth and absolute falsity values, specifically, some logics have $[[true]]_l < [[false]]_l$.
\end{enumerate}

\subsection{Sampler Interface and Implementation}

Based on these principles, we designed the \texttt{ABCSampler} interface in \texttt{\_samplers.py}.

Each backend provides a more specific abstract class that inherits from \\
\texttt{ABCSampler} and specifies the concrete types involved (e.g. \texttt{tf.Tensor} or\\
\texttt{torch.Tensor}). Additionally, each backend provides a default implementation called \texttt{[Pytorch, Tensorflow]DefaultSampler} that uses a fast gradient sign method to find adversarial examples. This default sampler is sufficient for many use cases and serves as a reference implementation since the step size is inferred from the bounds, specifically ensuring that the bounds of the search region can be reached. % TODO: reference to FGSM paper and preferrably also to one defining a general FGSM implementation/ step size definition

If a user wants to wants to use a custom sampling strategy, they can implement their own class conforming to \texttt{ABCSampler} and define a dictionary which maps the name of the variable being searched for to their corresponding sampler instance. This allows for having separate samplers for different quantified variables in the same specification, e.g. one for image perturbations and another for sensor noise.

\section{Extending and Aligning the translations}

With the abstractions and sampler design in place, the next step was to extend and align the \texttt{PythonTranslation} class to implement all of the necessary methods of \texttt{ABCTranslation}, and to do the same for both TensorFlow and the new PyTorch backend so that they implement \texttt{ABCBuiltins} and \texttt{ABCSampler} correctly. 

\subsection{Python translation}

The \texttt{PythonTranslation} class defines how Vehicle AST nodes are translated into Python \texttt{ast} nodes. The low-level operations are delegated to the backend-specific builtins and samplers by calling their corresponding methods from a dictionary that is assumed to be available at runtime.

In the case of the potentially user provided samplers, the availability of the specific sampler instance is checked at runtime by looking up the variable name in a \texttt{samplers} dictionary that is passed to the compiled loss function. If no sampler is defined for this variable, then a \texttt{KeyError} is returned. This is an intentional design choice to ensure that users are aware when they have not provided a sampler for a quantified variable, instead of defaulting to a sampler which may not be appropriate for their use case.

The translation's \texttt{compile} method generates python bytecode which then is executed with a decelaration context that includes the builtins and samplers, adding an entry to the provided dictionary containing the generated callable loss function. The method terminates by returning this updated dictionary containing the result of the execution.

\subsection{TensorFlow Backend}

The TensorFlow backend was updated to implement the new \texttt{ABCBuiltins} and \texttt{ABCSampler} interfaces:

\begin{itemize}
	\item Each Vehicle builtin is mapped to an appropriate TensorFlow operation or composition of operations.
	\item While the translation of the \texttt{SearchRatTensor} node is handled by\\
	\texttt{PythonTranslation}, the specific sampler interface and default implementation is defined in \texttt{tensorflow.samplers.py}. 
\end{itemize}

Most of the needed operations already existed in the original implementation, so the changes were mostly around updating method signatures and adding missing operations to align with the new abstractions.

\subsection{PyTorch Backend}

On the \texttt{dev} branch there was no PyTorch. On my branch I implemented a parallel backend that conforms to \texttt{ABCBuiltins} and \texttt{ABCSampler} for \texttt{torch.Tensor}:

\begin{itemize}
	\item Tensor operations and reductions mirror those of the TensorFlow backend, but are implemented using PyTorch's API.
	\item The sampler uses PyTorch tensors and operations to instantiate search regions and compute approximate worst-case losses for \texttt{SearchRatTensor}.
\end{itemize}

Since the abstractions were designed to be backend-agnostic, this implementation was straightforward, requiring mostly one-to-one mappings of operations from TensorFlow to PyTorch with minor differences in API conventions.

This ensures that the same Vehicle specification can be compiled into PyTorch-based loss functions, enabling property-driven training in PyTorch with no changes to the original specification. This was a key goal of the project, as it broadens the applicability of Vehicle to a wider range of projects. This also serves as a step towards integrating the work of \textit{Flinkow et al}\cite{flinkow2025property} within Vehicle.

\section{High-Level Python API and Package Design}

Finally, I integrated these internal changes into a high-level Python API aimed at end users. The primary entry point for training with constraints is the\\
\texttt{load\_specification} function:

\begin{itemize}
	\item It takes a Vehicle specification file, a choice of \texttt{LossBackend} (TensorFlow or PyTorch), and optionally a dictionary of custom samplers.
	\item Internally, it calls \texttt{\_ast.load} to obtain the updated AST, and then uses the appropriate backend-specific translation to populate a user-provided\\
	\texttt{declaration\_context} mapping.
	\item The resulting context contains callable loss functions for each named declaration (property) in the Vehicle specification, ready to be plugged into standard training loops.
\end{itemize}

The package exposes only the necessary components for users, while keeping the internal modules such as \texttt{\_ast}, \texttt{\_abc}, and backend implementations private. While a user can still access these internal modules if needed, the high-level API is designed to cover the common use cases without requiring direct interaction with the lower-level details. This also serves to make it easier to understand which parts of the codebase are stable and intended for public use versus those that are more experimental or subject to change. 

This design keeps the internal complexity of AST decoding, abstraction, and sampler semantics hidden from users, while still allowing advanced users to override or extend sampler behaviour when needed. It also makes it more obvious which components are meant to be extended or customised, versus those that are would take more changes, such as adding a new backend.
